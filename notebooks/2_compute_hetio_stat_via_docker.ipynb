{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c319ac8",
   "metadata": {
    "papermill": {
     "duration": 0.009475,
     "end_time": "2025-11-21T15:47:47.663392",
     "exception": false,
     "start_time": "2025-11-21T15:47:47.653917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Compute DWPC for Dataset 2 (HPC Subset)\n",
    "\n",
    "This notebook computes Degree-Weighted Path Counts (DWPC) for a subset of Dataset 2 files:\n",
    "- 2016 real data (1 file)\n",
    "- 2016 permutation 001 (1 file)\n",
    "- 2016 random 001 (1 file)\n",
    "- 2024 real data (1 file)\n",
    "- 2024 permutation 001 (1 file)\n",
    "- 2024 random 001 (1 file)\n",
    "\n",
    "**Total: 6 datasets**\n",
    "\n",
    "**Expected runtime:** 2-3 hours\n",
    "\n",
    "**Outputs:**\n",
    "- 6 DWPC result CSVs in `output/dwpc_com/dataset2/results/`\n",
    "- 6 histograms in `output/dwpc_com/dataset2/histograms/`\n",
    "- Intermediate parquet files in `output/dwpc_com/dataset2/metapaths_parquet/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda40397",
   "metadata": {
    "papermill": {
     "duration": 0.006647,
     "end_time": "2025-11-21T15:47:47.677608",
     "exception": false,
     "start_time": "2025-11-21T15:47:47.670961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup Hetionet Docker API\n",
    "\n",
    "Before running this notebook, ensure the Hetionet Docker API is running:\n",
    "\n",
    "```bash\n",
    "cd connectivity-search-backend\n",
    "sudo ./run_stack.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83186d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:47:47.697202Z",
     "iopub.status.busy": "2025-11-21T15:47:47.696704Z",
     "iopub.status.idle": "2025-11-21T15:47:50.773650Z",
     "shell.execute_reply": "2025-11-21T15:47:50.772690Z"
    },
    "papermill": {
     "duration": 3.087094,
     "end_time": "2025-11-21T15:47:50.774698",
     "exception": false,
     "start_time": "2025-11-21T15:47:47.687604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "import os\nimport sys\nimport time\nimport logging\nimport warnings\nfrom pathlib import Path\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nos.environ['CONNECTIVITY_SEARCH_API'] = 'http://localhost:8015'\nos.environ['NEO4J_HOST'] = 'localhost'\n\nsrc_path = str(Path.cwd().parent / 'src')\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\nfrom dwpc_api import run_metapaths_for_df, validate_parquet_files\n\nQUIET_MODE = True\n\nif QUIET_MODE:\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=r\"pandas\\..*\")\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=r\"pyarrow\\..*\")\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=r\"httpx\\..*\")\n    warnings.filterwarnings(\"ignore\", category=FutureWarning, module=r\"urllib3\\..*\")\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=r\"tqdm\\..*\")\n    for name in (\"httpx\", \"urllib3\", \"chardet\"):\n        logging.getLogger(name).setLevel(logging.WARNING)\n\nprint(\"Imports loaded successfully\")\nprint(\"DWPC API functions imported from src/dwpc_api.py\")"
  },
  {
   "cell_type": "markdown",
   "id": "5f11f6cc",
   "metadata": {
    "papermill": {
     "duration": 0.004227,
     "end_time": "2025-11-21T15:47:50.783977",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.779750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Safety Check Configuration\n",
    "\n",
    "Data quality checks to detect failures early and prevent wasted computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6aaf30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:47:50.794906Z",
     "iopub.status.busy": "2025-11-21T15:47:50.794578Z",
     "iopub.status.idle": "2025-11-21T15:47:50.798842Z",
     "shell.execute_reply": "2025-11-21T15:47:50.798182Z"
    },
    "papermill": {
     "duration": 0.010413,
     "end_time": "2025-11-21T15:47:50.799673",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.789260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety check configuration:\n",
      "  Early validation: ENABLED (check first 10 pairs)\n",
      "  Periodic monitoring: ENABLED (every 100 pairs)\n",
      "  Final validation: ENABLED (min 20% completion)\n"
     ]
    }
   ],
   "source": [
    "ENABLE_EARLY_VALIDATION = True\n",
    "ENABLE_PERIODIC_MONITORING = True\n",
    "ENABLE_FINAL_VALIDATION = True\n",
    "\n",
    "EARLY_CHECK_THRESHOLD = 10\n",
    "PERIODIC_CHECK_INTERVAL = 100\n",
    "PERIODIC_FAILURE_THRESHOLD = 0.5\n",
    "FINAL_MIN_COMPLETION_RATE = 0.2\n",
    "\n",
    "print(\"Safety check configuration:\")\n",
    "print(f\"  Early validation: {'ENABLED' if ENABLE_EARLY_VALIDATION else 'DISABLED'} (check first {EARLY_CHECK_THRESHOLD} pairs)\")\n",
    "print(f\"  Periodic monitoring: {'ENABLED' if ENABLE_PERIODIC_MONITORING else 'DISABLED'} (every {PERIODIC_CHECK_INTERVAL} pairs)\")\n",
    "print(f\"  Final validation: {'ENABLED' if ENABLE_FINAL_VALIDATION else 'DISABLED'} (min {FINAL_MIN_COMPLETION_RATE:.0%} completion)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768710c",
   "metadata": {
    "papermill": {
     "duration": 0.004191,
     "end_time": "2025-11-21T15:47:50.808227",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.804036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Define 6 datasets to process:\n",
    "- 2 real datasets (2016 and 2024)\n",
    "- 2 permutation datasets (perm_001 for each year)\n",
    "- 2 random datasets (random_001 for each year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b0bc5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:47:50.818756Z",
     "iopub.status.busy": "2025-11-21T15:47:50.818519Z",
     "iopub.status.idle": "2025-11-21T15:47:50.829884Z",
     "shell.execute_reply": "2025-11-21T15:47:50.828403Z"
    },
    "papermill": {
     "duration": 0.019761,
     "end_time": "2025-11-21T15:47:50.831912",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.812151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 6 datasets for processing\n",
      "  - 2016: 1 real + 1 permutation + 1 random = 3 datasets\n",
      "  - 2024: 1 real + 1 permutation + 1 random = 3 datasets\n",
      "  - Total: 6 datasets\n"
     ]
    }
   ],
   "source": [
    "datasets_to_process = [\n",
    "    # 2016 Real\n",
    "    {\n",
    "        'name': 'dataset2_2016_real',\n",
    "        'path': '../output/intermediate/hetio_bppg_dataset2_filtered.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_target_id',\n",
    "        'year': 2016,\n",
    "        'type': 'real'\n",
    "    },\n",
    "    # 2016 Permutation 001\n",
    "    {\n",
    "        'name': 'dataset2_2016_perm_001',\n",
    "        'path': '../output/permutations/dataset2_2016/perm_001.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_target_id',\n",
    "        'year': 2016,\n",
    "        'type': 'permuted'\n",
    "    },\n",
    "    # 2016 Random 001\n",
    "    {\n",
    "        'name': 'dataset2_2016_random_001',\n",
    "        'path': '../output/random_samples/dataset2_2016/random_001.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_pseudo_target_id',\n",
    "        'year': 2016,\n",
    "        'type': 'random'\n",
    "    },\n",
    "    # 2024 Real\n",
    "    {\n",
    "        'name': 'dataset2_2024_real',\n",
    "        'path': '../output/intermediate/hetio_bppg_dataset2_2024_filtered.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_target_id',\n",
    "        'year': 2024,\n",
    "        'type': 'real'\n",
    "    },\n",
    "    # 2024 Permutation 001\n",
    "    {\n",
    "        'name': 'dataset2_2024_perm_001',\n",
    "        'path': '../output/permutations/dataset2_2024/perm_001.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_target_id',\n",
    "        'year': 2024,\n",
    "        'type': 'permuted'\n",
    "    },\n",
    "    # 2024 Random 001\n",
    "    {\n",
    "        'name': 'dataset2_2024_random_001',\n",
    "        'path': '../output/random_samples/dataset2_2024/random_001.csv',\n",
    "        'col_source': 'neo4j_source_id',\n",
    "        'col_target': 'neo4j_pseudo_target_id',\n",
    "        'year': 2024,\n",
    "        'type': 'random'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(datasets_to_process)} datasets for processing\")\n",
    "print(f\"  - 2016: 1 real + 1 permutation + 1 random = 3 datasets\")\n",
    "print(f\"  - 2024: 1 real + 1 permutation + 1 random = 3 datasets\")\n",
    "print(f\"  - Total: 6 datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb99b4",
   "metadata": {
    "papermill": {
     "duration": 0.007061,
     "end_time": "2025-11-21T15:47:50.841925",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.834864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Batch Processing Loop\n",
    "\n",
    "Process all 6 datasets sequentially. Each dataset will:\n",
    "1. Load GO-gene pairs from CSV\n",
    "2. Compute DWPC for all unique pairs via async API calls\n",
    "3. Merge parquet results and save to CSV\n",
    "4. Generate histogram (if 'dwpc' column exists)\n",
    "\n",
    "Expected runtime: 2-3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7caa0baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T15:47:50.854943Z",
     "iopub.status.busy": "2025-11-21T15:47:50.854660Z",
     "iopub.status.idle": "2025-11-21T20:33:53.476102Z",
     "shell.execute_reply": "2025-11-21T20:33:53.473771Z"
    },
    "papermill": {
     "duration": 17162.648985,
     "end_time": "2025-11-21T20:33:53.497625",
     "exception": false,
     "start_time": "2025-11-21T15:47:50.848640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING BATCH PROCESSING: 6 datasets\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Dataset 1/6: dataset2_2016_real\n",
      "  Year: 2016, Type: real\n",
      "  Path: ../output/intermediate/hetio_bppg_dataset2_filtered.csv\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT: Dataset already processed (1,521,268 rows)\n",
      "  Skipping to next dataset...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Dataset 2/6: dataset2_2016_perm_001\n",
      "  Year: 2016, Type: permuted\n",
      "  Path: ../output/permutations/dataset2_2016/perm_001.csv\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT: Dataset already processed (1,543,256 rows)\n",
      "  Skipping to next dataset...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Dataset 3/6: dataset2_2016_random_001\n",
      "  Year: 2016, Type: random\n",
      "  Path: ../output/random_samples/dataset2_2016/random_001.csv\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT: Dataset already processed (1,524,065 rows)\n",
      "  Skipping to next dataset...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Dataset 4/6: dataset2_2024_real\n",
      "  Year: 2024, Type: real\n",
      "  Path: ../output/intermediate/hetio_bppg_dataset2_2024_filtered.csv\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT: Dataset already processed (1,022,372 rows)\n",
      "  Skipping to next dataset...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Dataset 5/6: dataset2_2024_perm_001\n",
      "  Year: 2024, Type: permuted\n",
      "  Path: ../output/permutations/dataset2_2024/perm_001.csv\n",
      "================================================================================\n",
      "\n",
      "Loaded 19,661 GO-gene pairs (19,661 unique)\n",
      "\n",
      "Checking Neo4j health at http://localhost:8015/v1/nodes/...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Neo4j is healthy!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT: Found 15276 existing pair results\n",
      "  Skipping 15276 completed pairs\n",
      "  Processing 4385 remaining pairs\n",
      "Connection pooling configuration:\n",
      "  Max connections: 250\n",
      "  Keepalive connections: 200\n",
      "  Global concurrency: 120\n",
      "  Timeout: 90.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:   0%|                                                                                      | 0/4385 [00:00<?, ?task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:   3%|██                                                                          | 120/4385 [01:13<43:31,  1.63task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:   5%|████▏                                                                       | 240/4385 [02:43<48:00,  1.44task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:   8%|██████▏                                                                     | 360/4385 [04:12<48:01,  1.40task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  11%|████████▎                                                                   | 480/4385 [05:36<46:09,  1.41task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  14%|██████████▍                                                                 | 600/4385 [07:00<44:34,  1.42task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  16%|████████████▍                                                               | 720/4385 [08:23<42:48,  1.43task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  19%|██████████████▌                                                             | 840/4385 [09:42<40:36,  1.45task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  22%|████████████████▋                                                           | 960/4385 [11:00<38:33,  1.48task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  25%|██████████████████▍                                                        | 1080/4385 [12:18<36:41,  1.50task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  27%|████████████████████▌                                                      | 1200/4385 [13:46<36:27,  1.46task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  30%|██████████████████████▌                                                    | 1320/4385 [15:12<35:35,  1.44task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  33%|████████████████████████▋                                                  | 1440/4385 [16:45<35:21,  1.39task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  36%|██████████████████████████▋                                                | 1560/4385 [18:19<34:52,  1.35task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  38%|████████████████████████████▋                                              | 1680/4385 [19:43<32:47,  1.38task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  41%|██████████████████████████████▊                                            | 1800/4385 [21:12<31:28,  1.37task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  44%|████████████████████████████████▊                                          | 1920/4385 [22:46<30:40,  1.34task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  47%|██████████████████████████████████▉                                        | 2040/4385 [24:23<29:56,  1.31task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  49%|████████████████████████████████████▉                                      | 2160/4385 [25:52<28:06,  1.32task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  52%|██████████████████████████████████████▉                                    | 2280/4385 [27:18<26:09,  1.34task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  55%|█████████████████████████████████████████                                  | 2400/4385 [28:49<24:47,  1.33task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  57%|███████████████████████████████████████████                                | 2520/4385 [30:20<23:24,  1.33task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  60%|█████████████████████████████████████████████▏                             | 2640/4385 [32:00<22:36,  1.29task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  63%|███████████████████████████████████████████████▏                           | 2760/4385 [33:32<20:57,  1.29task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  66%|█████████████████████████████████████████████████▎                         | 2880/4385 [35:01<19:10,  1.31task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  68%|███████████████████████████████████████████████████▎                       | 3000/4385 [36:36<17:50,  1.29task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  71%|█████████████████████████████████████████████████████▎                     | 3120/4385 [38:01<15:53,  1.33task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  74%|███████████████████████████████████████████████████████▍                   | 3240/4385 [39:14<13:33,  1.41task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  77%|█████████████████████████████████████████████████████████▍                 | 3360/4385 [40:26<11:33,  1.48task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  79%|███████████████████████████████████████████████████████████▌               | 3480/4385 [41:42<10:00,  1.51task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  82%|█████████████████████████████████████████████████████████████▌             | 3600/4385 [42:54<08:24,  1.56task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  85%|███████████████████████████████████████████████████████████████▋           | 3720/4385 [44:09<07:03,  1.57task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  88%|█████████████████████████████████████████████████████████████████▋         | 3840/4385 [45:24<05:46,  1.57task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  90%|███████████████████████████████████████████████████████████████████▋       | 3960/4385 [46:35<04:24,  1.61task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  93%|█████████████████████████████████████████████████████████████████████▊     | 4080/4385 [47:53<03:12,  1.58task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  96%|███████████████████████████████████████████████████████████████████████▊   | 4200/4385 [49:11<01:57,  1.57task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]:  99%|█████████████████████████████████████████████████████████████████████████▉ | 4320/4385 [50:23<00:40,  1.60task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]: 100%|███████████████████████████████████████████████████████████████████████████| 4385/4385 [50:53<00:00,  1.68task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing [dataset2_2024_perm_001]: 100%|███████████████████████████████████████████████████████████████████████████| 4385/4385 [50:53<00:00,  1.44task/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Path('../output/dwpc_com/dataset2/results').mkdir(parents=True, exist_ok=True)\n",
    "Path('../output/dwpc_com/dataset2/histograms').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_start_time = time.perf_counter()\n",
    "batch_summary = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"STARTING BATCH PROCESSING: {len(datasets_to_process)} datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    for i, dataset in enumerate(datasets_to_process, 1):\n",
    "        dataset_start = time.perf_counter()\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Dataset {i}/{len(datasets_to_process)}: {dataset['name']}\")\n",
    "        print(f\"  Year: {dataset['year']}, Type: {dataset['type']}\")\n",
    "        print(f\"  Path: {dataset['path']}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "        csv_path = f'../output/dwpc_com/dataset2/results/res_{dataset[\"name\"]}.csv'\n",
    "\n",
    "        if Path(csv_path).exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(csv_path)\n",
    "                if len(existing_df) > 0:\n",
    "                    print(f\"CHECKPOINT: Dataset already processed ({len(existing_df):,} rows)\")\n",
    "                    print(f\"  Skipping to next dataset...\\n\")\n",
    "                    batch_summary.append({\n",
    "                        'dataset_number': i,\n",
    "                        'dataset_name': dataset['name'],\n",
    "                        'year': dataset['year'],\n",
    "                        'type': dataset['type'],\n",
    "                        'n_input_pairs': None,\n",
    "                        'n_output_rows': len(existing_df),\n",
    "                        'time_seconds': 0,\n",
    "                        'status': 'skipped (already complete)'\n",
    "                    })\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read existing CSV: {e}\")\n",
    "                print(f\"  Will reprocess dataset...\\n\")\n",
    "\n",
    "        df = pd.read_csv(dataset['path'])\n",
    "        n_pairs = len(df)\n",
    "        n_unique_pairs = len(df[[dataset['col_source'], dataset['col_target']]].dropna().drop_duplicates())\n",
    "        print(f\"Loaded {n_pairs:,} GO-gene pairs ({n_unique_pairs:,} unique)\\n\")\n",
    "\n",
    "        max_attempts = 3\n",
    "        for attempt in range(1, max_attempts + 1):\n",
    "            if attempt > 1:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"RETRY ATTEMPT {attempt}/{max_attempts}\")\n",
    "                print(f\"{'='*60}\\n\")\n",
    "\n",
    "            summary = await run_metapaths_for_df(\n",
    "                df,\n",
    "                col_source=dataset['col_source'],\n",
    "                col_target=dataset['col_target'],\n",
    "                base_out_dir=Path('../output/dwpc_com/dataset2/metapaths_parquet'),\n",
    "                group=dataset['name'],\n",
    "                clear_group=False,\n",
    "                max_concurrency=120,\n",
    "                retries=10,\n",
    "                backoff_first=10.0,\n",
    "            )\n",
    "\n",
    "            parquet_dir = Path(f\"../output/dwpc_com/dataset2/metapaths_parquet/{dataset['name']}\")\n",
    "            parquet_files = list(parquet_dir.glob(\"*.parquet\"))\n",
    "\n",
    "            if not parquet_files:\n",
    "                raise FileNotFoundError(f\"No parquet files found in {parquet_dir}\")\n",
    "\n",
    "            n_completed = len(parquet_files)\n",
    "            completion_rate = (n_completed / n_unique_pairs) * 100\n",
    "\n",
    "            print(f\"\\nCompletion check:\")\n",
    "            print(f\"  Expected pairs: {n_unique_pairs:,}\")\n",
    "            print(f\"  Completed pairs: {n_completed:,}\")\n",
    "            print(f\"  Completion rate: {completion_rate:.2f}%\")\n",
    "\n",
    "            if n_completed == n_unique_pairs:\n",
    "                print(f\"  Status: COMPLETE\")\n",
    "                break\n",
    "            else:\n",
    "                missing = n_unique_pairs - n_completed\n",
    "                print(f\"  Status: INCOMPLETE ({missing:,} pairs missing)\")\n",
    "                \n",
    "                if attempt < max_attempts:\n",
    "                    print(f\"  Will retry missing pairs (attempt {attempt + 1}/{max_attempts})...\")\n",
    "                else:\n",
    "                    print(f\"  WARNING: Maximum retry attempts reached\")\n",
    "                    print(f\"  Proceeding with {n_completed:,} of {n_unique_pairs:,} pairs\")\n",
    "\n",
    "        parquet_files = sorted(parquet_dir.glob(\"*.parquet\"))\n",
    "        \n",
    "        if ENABLE_FINAL_VALIDATION:\n",
    "            try:\n",
    "                n_valid, n_empty, completion_rate = validate_parquet_files(\n",
    "                    parquet_dir, \n",
    "                    n_unique_pairs, \n",
    "                    min_completion_rate=FINAL_MIN_COMPLETION_RATE\n",
    "                )\n",
    "                print(f\"\\nFinal validation passed:\")\n",
    "                print(f\"  Valid files: {n_valid}\")\n",
    "                print(f\"  Empty files: {n_empty}\")\n",
    "                print(f\"  Completion rate: {completion_rate:.1%}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"\\nFinal validation failed - skipping merge\")\n",
    "                batch_summary.append({\n",
    "                    'dataset_number': i,\n",
    "                    'dataset_name': dataset['name'],\n",
    "                    'year': dataset['year'],\n",
    "                    'type': dataset['type'],\n",
    "                    'n_input_pairs': n_pairs,\n",
    "                    'n_unique_pairs': n_unique_pairs,\n",
    "                    'n_completed_pairs': len(parquet_files),\n",
    "                    'completion_pct': (len(parquet_files) / n_unique_pairs) * 100,\n",
    "                    'n_output_rows': 0,\n",
    "                    'time_seconds': time.perf_counter() - dataset_start,\n",
    "                    'status': 'validation failed'\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nMerging {len(parquet_files)} parquet files...\")\n",
    "        res_df = pd.concat(\n",
    "            [pd.read_parquet(fp) for fp in parquet_files],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        print(f\"  Merged {len(res_df):,} rows\")\n",
    "\n",
    "        res_df.to_csv(csv_path, index=False)\n",
    "        print(f\"  Saved: {csv_path}\")\n",
    "        \n",
    "        if 'dwpc' in res_df.columns:\n",
    "            dwpc_vals = res_df.loc[res_df[\"dwpc\"] > 0, \"dwpc\"].dropna()\n",
    "            \n",
    "            if len(dwpc_vals) > 0:\n",
    "                dwpc_mean = dwpc_vals.mean()\n",
    "                \n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.hist(dwpc_vals, bins=50, density=False, edgecolor=\"black\", linewidth=0.5)\n",
    "                plt.axvline(dwpc_mean, color=\"red\", linestyle=\"--\", linewidth=1.2, \n",
    "                           label=f\"Mean = {dwpc_mean:.2f}\")\n",
    "                plt.title(f\"DWPC Distribution: {dataset['name']}\", fontsize=12, weight=\"bold\")\n",
    "                plt.xlabel(\"DWPC\", fontsize=11)\n",
    "                plt.ylabel(\"Count\", fontsize=11)\n",
    "                plt.legend(fontsize=10)\n",
    "                plt.tick_params(axis=\"both\", labelsize=10)\n",
    "                plt.grid(axis=\"y\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                hist_path = f\"../output/dwpc_com/dataset2/histograms/hist_{dataset['name']}.png\"\n",
    "                plt.savefig(hist_path, dpi=300, bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "                print(f\"  Saved histogram: {hist_path}\")\n",
    "            else:\n",
    "                print(f\"  Skipping histogram: no positive DWPC values\")\n",
    "        else:\n",
    "            print(f\"  Skipping histogram: 'dwpc' column not found\")\n",
    "        \n",
    "        dataset_time = time.perf_counter() - dataset_start\n",
    "        \n",
    "        final_completion = (len(parquet_files) / n_unique_pairs) * 100\n",
    "        status = 'completed' if final_completion == 100.0 else f'incomplete ({final_completion:.1f}%)'\n",
    "        \n",
    "        batch_summary.append({\n",
    "            'dataset_number': i,\n",
    "            'dataset_name': dataset['name'],\n",
    "            'year': dataset['year'],\n",
    "            'type': dataset['type'],\n",
    "            'n_input_pairs': n_pairs,\n",
    "            'n_unique_pairs': n_unique_pairs,\n",
    "            'n_completed_pairs': len(parquet_files),\n",
    "            'completion_pct': final_completion,\n",
    "            'n_output_rows': len(res_df),\n",
    "            'time_seconds': dataset_time,\n",
    "            'status': status\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nCompleted {dataset['name']} in {dataset_time/60:.1f} minutes\")\n",
    "        \n",
    "        elapsed = time.perf_counter() - batch_start_time\n",
    "        avg_time_per_dataset = elapsed / i\n",
    "        remaining = (len(datasets_to_process) - i) * avg_time_per_dataset\n",
    "        print(f\"  Progress: {i}/{len(datasets_to_process)} datasets\")\n",
    "        print(f\"  Elapsed: {elapsed/3600:.2f} hours\")\n",
    "        print(f\"  Estimated remaining: {remaining/3600:.2f} hours\")\n",
    "    \n",
    "    batch_time = time.perf_counter() - batch_start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"  Total time: {batch_time/3600:.2f} hours\")\n",
    "    print(f\"  Datasets processed: {len(datasets_to_process)}\")\n",
    "    print(f\"  All results saved to: output/dwpc_com/dataset2/results/\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ERROR: Batch processing failed at dataset {i}: {dataset['name']}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    batch_summary.append({\n",
    "        'dataset_number': i,\n",
    "        'dataset_name': dataset['name'],\n",
    "        'year': dataset['year'],\n",
    "        'type': dataset['type'],\n",
    "        'n_input_pairs': None,\n",
    "        'n_unique_pairs': None,\n",
    "        'n_completed_pairs': None,\n",
    "        'completion_pct': None,\n",
    "        'n_output_rows': None,\n",
    "        'time_seconds': None,\n",
    "        'status': 'failed'\n",
    "    })\n",
    "    \n",
    "    raise\n",
    "\n",
    "batch_summary_df = pd.DataFrame(batch_summary)\n",
    "print(\"\\nBatch Processing Summary:\")\n",
    "print(batch_summary_df.to_string(index=False))\n",
    "\n",
    "incomplete_datasets = batch_summary_df[\n",
    "    (batch_summary_df['status'] != 'skipped (already complete)') & \n",
    "    (batch_summary_df['completion_pct'] < 100.0)\n",
    "]\n",
    "\n",
    "if len(incomplete_datasets) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WARNING: Some datasets are incomplete!\")\n",
    "    print(\"=\"*80)\n",
    "    for _, row in incomplete_datasets.iterrows():\n",
    "        print(f\"  {row['dataset_name']}: {row['completion_pct']:.1f}% complete \"\n",
    "              f\"({row['n_completed_pairs']}/{row['n_unique_pairs']} pairs)\")\n",
    "    print(\"\\nRecommendation: Rerun this notebook to fill gaps automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7201b9",
   "metadata": {
    "papermill": {
     "duration": 0.012743,
     "end_time": "2025-11-21T20:33:53.544735",
     "exception": false,
     "start_time": "2025-11-21T20:33:53.531992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Validation\n",
    "\n",
    "Verify that all 6 datasets were processed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8839b808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T20:33:53.570575Z",
     "iopub.status.busy": "2025-11-21T20:33:53.570446Z",
     "iopub.status.idle": "2025-11-21T20:33:57.894368Z",
     "shell.execute_reply": "2025-11-21T20:33:57.894118Z"
    },
    "papermill": {
     "duration": 4.337844,
     "end_time": "2025-11-21T20:33:57.896048",
     "exception": false,
     "start_time": "2025-11-21T20:33:53.558204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dir = Path('../output/dwpc_com/dataset2/results')\n",
    "histograms_dir = Path('../output/dwpc_com/dataset2/histograms')\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Dataset Name':<30} {'CSV Exists':<12} {'Rows':<12} {'Histogram':<12} {'Status':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_passed = True\n",
    "\n",
    "for dataset in datasets_to_process:\n",
    "    name = dataset['name']\n",
    "    csv_path = results_dir / f\"res_{name}.csv\"\n",
    "    hist_path = histograms_dir / f\"hist_{name}.png\"\n",
    "    \n",
    "    csv_exists = csv_path.exists()\n",
    "    hist_exists = hist_path.exists()\n",
    "    \n",
    "    if csv_exists:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            n_rows = len(df)\n",
    "            status = \"PASS\" if n_rows > 0 else \"FAIL\"\n",
    "        except Exception as e:\n",
    "            n_rows = \"ERROR\"\n",
    "            status = \"FAIL\"\n",
    "    else:\n",
    "        n_rows = \"N/A\"\n",
    "        status = \"FAIL\"\n",
    "    \n",
    "    csv_status = \"YES\" if csv_exists else \"NO\"\n",
    "    hist_status = \"YES\" if hist_exists else \"NO\"\n",
    "    \n",
    "    print(f\"{name:<30} {csv_status:<12} {str(n_rows):<12} {hist_status:<12} {status:<10}\")\n",
    "    \n",
    "    validation_results.append({\n",
    "        'dataset': name,\n",
    "        'csv_exists': csv_exists,\n",
    "        'n_rows': n_rows if csv_exists else 0,\n",
    "        'histogram_exists': hist_exists,\n",
    "        'status': status\n",
    "    })\n",
    "    \n",
    "    if status != \"PASS\":\n",
    "        all_passed = False\n",
    "\n",
    "print(\"-\"*80)\n",
    "if all_passed:\n",
    "    print(f\"\\nAll {len(datasets_to_process)} datasets processed successfully!\")\n",
    "    print(f\"  Results: {results_dir}\")\n",
    "    print(f\"  Histograms: {histograms_dir}\")\n",
    "else:\n",
    "    print(\"\\nSome datasets failed validation. Check output files.\")\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total datasets: {len(datasets_to_process)}\")\n",
    "print(f\"  Passed: {sum(1 for v in validation_results if v['status'] == 'PASS')}\")\n",
    "print(f\"  Failed: {sum(1 for v in validation_results if v['status'] == 'FAIL')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_dwpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17181.373004,
   "end_time": "2025-11-21T20:34:00.534124",
   "environment_variables": {},
   "exception": null,
   "input_path": "2_compute_hetio_stat_para_docker_api_HPC.ipynb",
   "output_path": "./executed/2_compute_hetio_stat_para_docker_api_HPC_executed.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T15:47:39.161120",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}